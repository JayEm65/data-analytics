{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc1_'></a>[Ensemble methods](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Ensemble**: Borrowed from French ensemble. A group of separate things that contribute to a coordinated whole. [$^{[1]}$](https://en.wiktionary.org/wiki/ensemble#French)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Table of contents**<a id='toc0_'></a>    \n",
        "- [Ensemble methods](#toc1_)    \n",
        "- [Baseline](#toc2_)    \n",
        "- [Voting classifiers](#toc3_)    \n",
        "- [Bagging (Bootstrap aggregation)](#toc4_)    \n",
        "  - [Bootstrapping](#toc4_1_)    \n",
        "  - [Pasting](#toc4_2_)    \n",
        "  - [Patching](#toc4_3_)    \n",
        "  - [Bagging in practice](#toc4_4_)    \n",
        "  - [Random Forest](#toc4_5_)    \n",
        "- [Boosting](#toc5_)    \n",
        "  - [Adaboost (Adaptive Boosting)](#toc5_1_)    \n",
        "  - [Gradient boosting](#toc5_2_)    \n",
        "  - [Extreme Gradient Boosting](#toc5_3_)    \n",
        "- [Resources](#toc6_)    \n",
        "- [References](#toc7_)    \n",
        "- [Acknowledgements](#toc8_)    \n",
        "\n",
        "<!-- vscode-jupyter-toc-config\n",
        "\tnumbering=false\n",
        "\tanchor=true\n",
        "\tflat=false\n",
        "\tminLevel=1\n",
        "\tmaxLevel=6\n",
        "\t/vscode-jupyter-toc-config -->\n",
        "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJ2LeEdzAKXQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "RBlzmGP-ynbn",
        "outputId": "5779a5ea-4889-4433-f5b3-8201dfaa348c"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "def load_boston_dataset():\n",
        "    dataset = fetch_openml(name='boston', version=1)\n",
        "    return dataset.data, dataset.target\n",
        "\n",
        "features, labels = load_boston_dataset()\n",
        "features = features.astype(float)\n",
        "\n",
        "display(features.head())\n",
        "display(labels.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remember, before we go to the train-test split we do data review, exploratory data analysis, and some feature engineering:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYMIjC9wAKXa"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.25, random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc2_'></a>[Baseline](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Here we are using all 506 observation (non-optimized method, straight from the shelf):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree = DecisionTreeRegressor(random_state=1) # fixing random state because I'm a chicken and terrified that random variation screws up my example\n",
        "tree.fit(X_train, y_train)\n",
        "print('Train score:', tree.score(X_train, y_train))\n",
        "print('Test score:', tree.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc3_'></a>[Voting classifiers](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> A Voting Classifier is a machine learning model that trains on an ensemble of numerous models and predicts an output (class) based on their highest probability of chosen class as the output. [$^{[2]}$](https://medium.com/@imamitsingh/voting-classifiers-in-machine-learning-a532935fe592)  \n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:1100/format:webp/0*Lp5aIdSuk4uqGNwO.png)  \n",
        "(Source: [Voting Classifiers in Machine Learning, Medium](https://medium.com/@imamitsingh/voting-classifiers-in-machine-learning-a532935fe592))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import VotingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "lin_reg = LinearRegression()\n",
        "knn_reg = KNeighborsRegressor()\n",
        "tree_reg = DecisionTreeRegressor(random_state=1)\n",
        "voting_reg = VotingRegressor(\n",
        "estimators=[('lr', lin_reg), ('dt', tree_reg), ('knn', knn_reg)])\n",
        "voting_reg.fit(X_train, y_train)\n",
        "print('Train score:', voting_reg.score(X_train, y_train))\n",
        "print('Test score:', voting_reg.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's have a look at the individual $R^2$ scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "knn_reg = KNeighborsRegressor()\n",
        "knn_reg.fit(X_train, y_train)\n",
        "print('Train score:', knn_reg.score(X_train, y_train))\n",
        "print('Test score:', knn_reg.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train, y_train)\n",
        "print('Train score:', lin_reg.score(X_train, y_train))\n",
        "print('Test score:', lin_reg.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The mix of 2 decent models and one not so decent model fares better than each of the models!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why not use the same model with different parameters in the same `VotingRegressor`?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** For Voting Classifiers, we have 2 types of voting:\n",
        "\n",
        "> - **Hard Voting** - make the final prediction by a simple majority vote for accuracy.   \n",
        "> - **Soft Voting** - averaging out the probabilities calculated by individual algorithms then choosing the class based on the overall decision boundary. This can only be done when all your classifiers can calculate probabilities for the outcomes. [$^{[3]}$](https://towardsdatascience.com/ensemble-learning-in-machine-learning-getting-started-4ed85eb38e00)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc4_'></a>[Bagging (Bootstrap aggregation)](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Bootstrap aggregating, also called bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting.  [$^{[4]}$](https://en.wikipedia.org/wiki/Bootstrap_aggregating) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://github.com/sabinagio/data-analytics/blob/main/images/bagging.png?raw=true)  \n",
        "(Source: [Bootstrap Aggregating, Wikipedia](https://en.wikipedia.org/wiki/Bootstrap_aggregating))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc4_1_'></a>[Bootstrapping](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> The bootstrap dataset is made by randomly picking objects from the original dataset. Also, it must be the same size as the original dataset. However, the difference is that the bootstrap dataset can have duplicate objects: [$^{[4]}$](https://en.wikipedia.org/wiki/Bootstrap_aggregating)   \n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/f/fe/Bootstrap_Example_2.png)  \n",
        "(Source: [Bootstrap aggregating, Wikipedia](https://en.wikipedia.org/wiki/Bootstrap_aggregating))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://upload.wikimedia.org/wikipedia/commons/5/57/Complete_Example_2.png)  \n",
        "(Source: [Bootstrap aggregating, Wikipedia](https://en.wikipedia.org/wiki/Bootstrap_aggregating))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc4_2_'></a>[Pasting](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> In case of Pasting, the same process as for Bootstrapping applies, only difference being that pasting doesn’t allow training instances to be sampled several times for the same predictors so the dataset sizes will not be the same as the original. [$^{[3]}$](https://archive.is/20210523073415/https://towardsdatascience.com/ensemble-learning-in-machine-learning-getting-started-4ed85eb38e00#selection-1085.149-1089.148)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc4_3_'></a>[Patching](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc4_4_'></a>[Bagging in practice](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Bagging means we train many \"weak\" predictors but then we combine their predictions and some will hopefully make up for the others' failures:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import BaggingRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbKgTxrIqr3S",
        "outputId": "c42e6538-0fdc-40ed-b675-feb2762bc622"
      },
      "outputs": [],
      "source": [
        "# Bagging w/ pasting\n",
        "bagging_reg = BaggingRegressor(\n",
        "    DecisionTreeRegressor(max_depth=3), # depth 3 to force tree to be \"weak\"\n",
        "    n_estimators=10, # 10 trees\n",
        "    max_samples=100, # we limit each weaker tree to 100 datapoints\n",
        "    bootstrap=False, # by default, the bagging regressor does bootstrapping\n",
        "    random_state=1) # fixing random state because I want my examples to work and to look smart\n",
        "\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "print('Train score:', bagging_reg.score(X_train, y_train))\n",
        "print('Test score:', bagging_reg.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bagging w/ bootstrapping\n",
        "bagging_reg = BaggingRegressor(\n",
        "    DecisionTreeRegressor(max_depth=3), # depth 3 to force tree to be \"weak\"\n",
        "    n_estimators=10, # 10 trees\n",
        "    max_samples=100, # we limit each weaker tree to 100 datapoints\n",
        "    bootstrap=True,  # by default, the bagging regressor does bootstrapping\n",
        "    random_state=1) # fixing random state because I want my examples to work and to look smart\n",
        "\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "print('Train score:', bagging_reg.score(X_train, y_train))\n",
        "print('Test score:', bagging_reg.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Which one to choose? Bootstrapping or Pasting?**\n",
        "\n",
        "It depends on the size of your dataset. \n",
        "\n",
        "> Since pasting is without replacement, each subset of the sample can be used once at most, which means that you need a big dataset for it to work. As a matter of fact, pasting was originally designed for large data-sets, when computing power is limited. Bagging, on the other hand, can use the same subsets many times, which is great for smaller sample sizes, in which it improves robustness. [$^{[5]}$](https://stats.stackexchange.com/questions/219193/when-should-the-pasting-ensemble-method-be-used-instead-of-bagging)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Note:** BaggingClassifier automatically performs soft voting if the classifier can calculate the probabilities for its predictions. [$^{[3]}$](https://archive.is/20210523073415/https://towardsdatascience.com/ensemble-learning-in-machine-learning-getting-started-4ed85eb38e00#selection-1085.149-1089.148)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc4_5_'></a>[Random Forest](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Random forests not only shuffle the dataset but also randomly select some features. They're using bagging with patching internally on decision trees so some trees will focus on one part of the data, some in others, then they meet to vote and get a holistic result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-TE0MMEqr59",
        "outputId": "52299bac-f30b-46df-f8d2-f2399b8ff2c5"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "forest = RandomForestRegressor(n_estimators=10, # same 10 trees\n",
        "                               #max_samples=100,\n",
        "                               #max_features=0.6,\n",
        "                               max_depth=3, # depth 3 to force tree to be \"weak\"\n",
        "                               random_state=1) # fixing rand because I'm insecure and afraid you will judge me if I get a bad random selection that does not prove my point\n",
        "forest.fit(X_train, y_train)\n",
        "print('Train score:', forest.score(X_train, y_train))\n",
        "print('Test score:', forest.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc5_'></a>[Boosting](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc5_1_'></a>[Adaboost (Adaptive Boosting)](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Adaboost \"directs\" each subsequent tree to focus on the datapoints that the preceding tree got wrong. This way the trees try to compensate for each others' weakenesses. \n",
        "\n",
        "Then, in order to mitigate the bias resulting from the trees learning from each other, the more \"naive\" trees (i.e. the first to be fit) have the highest weight in the final vote."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpCBIbyDqr8j",
        "outputId": "15cc1dcd-1f61-404a-e164-eb156b990de0"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "\n",
        "ada_reg = AdaBoostRegressor(DecisionTreeRegressor(max_depth=5), # you can overfit a bit because you compensate afterwards\n",
        "                            n_estimators=10, # same 10 trees. You usually use faaaar more estimators\n",
        "                            random_state=1 # once a coward, always a coward\n",
        "                            )\n",
        "ada_reg.fit(X_train, y_train)\n",
        "print('Train score:', ada_reg.score(X_train, y_train))\n",
        "print('Test score:', ada_reg.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Increasing the number of estimators:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-llI7h65y6mS"
      },
      "outputs": [],
      "source": [
        "ada_reg = AdaBoostRegressor(DecisionTreeRegressor(max_depth=5),\n",
        "                            n_estimators=50, # that's more like it\n",
        "                            random_state=1 # once a coward, always a coward\n",
        "                            )\n",
        "ada_reg.fit(X_train, y_train)\n",
        "print('Train score:', ada_reg.score(X_train, y_train))\n",
        "print('Test score:', ada_reg.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "AdaBoosting is a method of training a model rather than a separate model so it can be used with different weak estimators:\n",
        "\n",
        "> The base model used in boosting must be relatively low variance and high bias, but this is just a rule of thumb, boosting algorithms in python are usually implemented using decision trees by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ada_reg = AdaBoostRegressor(KNeighborsRegressor(),\n",
        "                            n_estimators=50, # that's more like it\n",
        "                            random_state=1 # once a coward, always a coward\n",
        "                            )\n",
        "ada_reg.fit(X_train, y_train)\n",
        "print('Train score:', ada_reg.score(X_train, y_train))\n",
        "print('Test score:', ada_reg.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Well, there's a reason we typically use these methods with Decision Trees :D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc5_2_'></a>[Gradient boosting](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Gradient boosting also focuses more on where the trees get it wrong but prefers to control the error rather than getting the observation fully right. It is really trying to just \"correct\" the preceeding tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DonfrQzXqr_L",
        "outputId": "cb77b51b-d0ea-4bc3-b9fa-a546ccf61919"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gb_reg = GradientBoostingRegressor(max_depth=5, #gradient boosting always works with trees, no need to call the tree regressor\n",
        "                                   n_estimators=50,\n",
        "                                   random_state=1 # tastes like chicken\n",
        "                                   )\n",
        "gb_reg.fit(X_train, y_train)\n",
        "gb_reg.score(X_test,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc5_3_'></a>[Extreme Gradient Boosting](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Evolution of tree-based algorithms](https://miro.medium.com/max/925/1*QJZ6W-Pck_W7RlIDwUIN9Q.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> XGBoost is one of the best algorithms to work on tabular data. This is not the only nor the best (IMO) implementation of XGB. For example, this does not accept NaNs (LightXGB would, for example) but still a champ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost\n",
        "\n",
        "xgb_reg = xgboost.XGBRegressor(max_depth=5)\n",
        "xgb_reg.fit(X_train, y_train)\n",
        "xgb_reg.score(X_test,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Imagine:** What could we have acheived if we had done feature engineering?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc6_'></a>[Resources](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- StatQuest\n",
        "    - [Bootstrapping (10 min)](https://www.youtube.com/watch?v=Xz0x-8-cgaQ) - to understand the random forest video below \n",
        "    - [Random Forests Part 1: Building, Using and Evaluating (10 min)](https://www.youtube.com/watch?v=J4Wdy0Wc_xQ)\n",
        "    - [Random Forests Part 2: Missing data and clustering (12 min)](https://www.youtube.com/watch?v=sQ870aTKqiM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc7_'></a>[References](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[1] [Ensemble, Wikipedia](https://en.wiktionary.org/wiki/ensemble#French)   \n",
        "[2] [Voting Classifiers in Machine Learning, Medium](https://medium.com/@imamitsingh/voting-classifiers-in-machine-learning-a532935fe592)  \n",
        "[3] [Ensemble Learning in Machine Learning, Towards Data Science](https://towardsdatascience.com/ensemble-learning-in-machine-learning-getting-started-4ed85eb38e00)  \n",
        "[4] [Bootstrap Aggregating, Wikipedia](https://en.wikipedia.org/wiki/Bootstrap_aggregating)  \n",
        "[5] [When should pasting be used instead of bagging, StackOverflow](https://stats.stackexchange.com/questions/219193/when-should-the-pasting-ensemble-method-be-used-instead-of-bagging)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc8_'></a>[Acknowledgements](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Thank you, David Henriques, for your awesome lesson structure and content!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
